@article{fraser2018,
    doi = {10.1371/journal.pone.0200303},
    author = {Fraser, Hannah AND Parker, Tim AND Nakagawa, Shinichi AND Barnett, Ashley AND Fidler, Fiona},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Questionable research practices in ecology and evolution},
    year = {2018},
    month = {07},
    volume = {13},
    url = {https://doi.org/10.1371/journal.pone.0200303},
    pages = {1-16},
    abstract = {We surveyed 807 researchers (494 ecologists and 313 evolutionary biologists) about their use of Questionable Research Practices (QRPs), including cherry picking statistically significant results, p hacking, and hypothesising after the results are known (HARKing). We also asked them to estimate the proportion of their colleagues that use each of these QRPs. Several of the QRPs were prevalent within the ecology and evolution research community. Across the two groups, we found 64% of surveyed researchers reported they had at least once failed to report results because they were not statistically significant (cherry picking); 42% had collected more data after inspecting whether results were statistically significant (a form of p hacking) and 51% had reported an unexpected finding as though it had been hypothesised from the start (HARKing). Such practices have been directly implicated in the low rates of reproducible results uncovered by recent large scale replication studies in psychology and other disciplines. The rates of QRPs found in this study are comparable with the rates seen in psychology, indicating that the reproducibility problems discovered in psychology are also likely to be present in ecology and evolution.},
    number = {7},

}

@article{brown2011,
author = {Brown, Christopher J. and Schoeman, David S. and Sydeman, William J. and Brander, Keith and Buckley, Lauren B. and Burrows, Michael and Duarte, Carlos M. and Moore, Pippa J. and Pandolfi, John M. and Poloczanska, Elvira and Venables, William and Richardson, Anthony J.},
title = {Quantitative approaches in climate change ecology},
journal = {Global Change Biology},
volume = {17},
number = {12},
pages = {3697-3713},
doi = {https://doi.org/10.1111/j.1365-2486.2011.02531.x},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1365-2486.2011.02531.x},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1365-2486.2011.02531.x},
abstract = {Abstract Contemporary impacts of anthropogenic climate change on ecosystems are increasingly being recognized. Documenting the extent of these impacts requires quantitative tools for analyses of ecological observations to distinguish climate impacts in noisy data and to understand interactions between climate variability and other drivers of change. To assist the development of reliable statistical approaches, we review the marine climate change literature and provide suggestions for quantitative approaches in climate change ecology. We compiled 267 peer-reviewed articles that examined relationships between climate change and marine ecological variables. Of the articles with time series data (n = 186), 75\% used statistics to test for a dependency of ecological variables on climate variables. We identified several common weaknesses in statistical approaches, including marginalizing other important non-climate drivers of change, ignoring temporal and spatial autocorrelation, averaging across spatial patterns and not reporting key metrics. We provide a list of issues that need to be addressed to make inferences more defensible, including the consideration of (i) data limitations and the comparability of data sets; (ii) alternative mechanisms for change; (iii) appropriate response variables; (iv) a suitable model for the process under study; (v) temporal autocorrelation; (vi) spatial autocorrelation and patterns; and (vii) the reporting of rates of change. While the focus of our review was marine studies, these suggestions are equally applicable to terrestrial studies. Consideration of these suggestions will help advance global knowledge of climate impacts and understanding of the processes driving ecological change.},
year = {2011}
}

@article{forstmeier2017,
author = {Forstmeier, Wolfgang and Wagenmakers, Eric-Jan and Parker, Timothy H.},
title = {Detecting and avoiding likely false-positive findings – a practical guide},
journal = {Biological Reviews},
volume = {92},
number = {4},
pages = {1941-1968},
keywords = {confirmation bias, HARKing, hindsight bias, overfitting, P-hacking, power, preregistration, replication, researcher degrees of freedom, Type I error},
doi = {https://doi.org/10.1111/brv.12315},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/brv.12315},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/brv.12315},
abstract = {ABSTRACT Recently there has been a growing concern that many published research findings do not hold up in attempts to replicate them. We argue that this problem may originate from a culture of ‘you can publish if you found a significant effect’. This culture creates a systematic bias against the null hypothesis which renders meta-analyses questionable and may even lead to a situation where hypotheses become difficult to falsify. In order to pinpoint the sources of error and possible solutions, we review current scientific practices with regard to their effect on the probability of drawing a false-positive conclusion. We explain why the proportion of published false-positive findings is expected to increase with (i) decreasing sample size, (ii) increasing pursuit of novelty, (iii) various forms of multiple testing and researcher flexibility, and (iv) incorrect P-values, especially due to unaccounted pseudoreplication, i.e. the non-independence of data points (clustered data). We provide examples showing how statistical pitfalls and psychological traps lead to conclusions that are biased and unreliable, and we show how these mistakes can be avoided. Ultimately, we hope to contribute to a culture of ‘you can publish if your study is rigorous’. To this end, we highlight promising strategies towards making science more objective. Specifically, we enthusiastically encourage scientists to preregister their studies (including a priori hypotheses and complete analysis plans), to blind observers to treatment groups during data collection and analysis, and unconditionally to report all results. Also, we advocate reallocating some efforts away from seeking novelty and discovery and towards replicating important research findings of one's own and of others for the benefit of the scientific community as a whole. We believe these efforts will be aided by a shift in evaluation criteria away from the current system which values metrics of ‘impact’ almost exclusively and towards a system which explicitly values indices of scientific rigour.},
year = {2017}
}

@article{warton2016,
author = {Warton, David I. and Lyons, Mitchell and Stoklosa, Jakub and Ives, Anthony R.},
title = {Three points to consider when choosing a LM or GLM test for count data},
journal = {Methods in Ecology and Evolution},
volume = {7},
number = {8},
pages = {882-890},
keywords = {data transformation, generalized linear models, multivariate analysis, power analysis, type I error},
doi = {https://doi.org/10.1111/2041-210X.12552},
url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.12552},
eprint = {https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.12552},
abstract = {Summary The two most common approaches for analysing count data are to use a generalized linear model (GLM), or transform data, and use a linear model (LM). The latter has recently been advocated to more reliably maintain control of type I error rates in tests for no association, while seemingly losing little in power. We make three points on this issue. Point 1 – Choice of statistical model should primarily be made on the grounds of data properties. Choice of testing procedure should be considered and addressed as a separate issue, after model choice. If models with the appropriate data properties nonetheless have statistical problems such as type I error control (i.e. type I error rate greatly exceeds the intended significance level), the best solution is to keep the model but fix the problems. Point 2 – When a test has problems with type I error control, it can usually be corrected, but this may require departure from software default approaches. In particular, resampling is a good solution for small samples that can be easy to implement. Point 3 –Tests based on models that better fit the data (e.g. a negative binomial for overdispersed count data) tend to have better power properties and in some instances have considerably higher power. We illustrate these issues for a 2 × 2 experiment with a count response. This seemingly simple problem becomes hard when the experimental design is unbalanced, and software default procedures using LMs or GLMs can have difficulties, although in both cases the issues can be fixed. We conclude that, when GLMs are thought to fit count data well, and when any necessary steps are taken to correct type I error rates, they should be used rather than LMs. Nonetheless, standard LM tests are often robust and can have good type I error control, so there is an argument for their use for counts when diagnostics are difficult and statistical models are complex, although at some risk of loss of power and interpretability.},
year = {2016}
}

@article{ohara2010,
author = {O’Hara, Robert B. and Kotze, D. Johan},
title = {Do not log-transform count data},
journal = {Methods in Ecology and Evolution},
volume = {1},
number = {2},
pages = {118-122},
keywords = {generalized linear models, linear models, overdispersion, Poisson, transformation},
doi = {https://doi.org/10.1111/j.2041-210X.2010.00021.x},
url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/j.2041-210X.2010.00021.x},
eprint = {https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2041-210X.2010.00021.x},
abstract = {Summary 1. Ecological count data (e.g. number of individuals or species) are often log-transformed to satisfy parametric test assumptions. 2. Apart from the fact that generalized linear models are better suited in dealing with count data, a log-transformation of counts has the additional quandary in how to deal with zero observations. With just one zero observation (if this observation represents a sampling unit), the whole data set needs to be fudged by adding a value (usually 1) before transformation. 3. Simulating data from a negative binomial distribution, we compared the outcome of fitting models that were transformed in various ways (log, square root) with results from fitting models using quasi-Poisson and negative binomial models to untransformed count data. 4. We found that the transformations performed poorly, except when the dispersion was small and the mean counts were large. The quasi-Poisson and negative binomial models consistently performed well, with little bias. 5. We recommend that count data should not be analysed by log-transforming it, but instead models based on Poisson and negative binomial distributions should be used.},
year = {2010}
}

@article{arif2022,
author = {Arif, Suchinta and MacNeil, M. Aaron},
title = {Predictive models aren't for causal inference},
journal = {Ecology Letters},
volume = {25},
number = {8},
pages = {1741-1745},
keywords = {back-door criterion, causal inference, directed acyclic graphs (DAGs), model selection, prediction},
doi = {https://doi.org/10.1111/ele.14033},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ele.14033},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/ele.14033},
abstract = {Abstract Ecologists often rely on observational data to understand causal relationships. Although observational causal inference methodologies exist, predictive techniques such as model selection based on information criterion (e.g. AIC) remains a common approach used to understand ecological relationships. However, predictive approaches are not appropriate for drawing causal conclusions. Here, we highlight the distinction between predictive and causal inference and show how predictive techniques can lead to biased causal estimates. Instead, we encourage ecologists to valid causal inference methods such as the backdoor criterion, a graphical rule that can be used to determine causal relationships across observational studies.},
year = {2022}
}


@Article{bolker2024,
AUTHOR = {Bolker, Benjamin M.},
TITLE = {Multimodel Approaches Are Not the Best Way to Understand Multifactorial Systems},
JOURNAL = {Entropy},
VOLUME = {26},
YEAR = {2024},
NUMBER = {6},
ARTICLE-NUMBER = {506},
URL = {https://www.mdpi.com/1099-4300/26/6/506},
PubMedID = {38920515},
ISSN = {1099-4300},
ABSTRACT = {Information-theoretic (IT) and multi-model averaging (MMA) statistical approaches are widely used but suboptimal tools for pursuing a multifactorial approach (also known as the method of multiple working hypotheses) in ecology. (1) Conceptually, IT encourages ecologists to perform tests on sets of artificially simplified models. (2) MMA improves on IT model selection by implementing a simple form of shrinkage estimation (a way to make accurate predictions from a model with many parameters relative to the amount of data, by “shrinking” parameter estimates toward zero). However, other shrinkage estimators such as penalized regression or Bayesian hierarchical models with regularizing priors are more computationally efficient and better supported theoretically. (3) In general, the procedures for extracting confidence intervals from MMA are overconfident, providing overly narrow intervals. If researchers want to use limited data sets to accurately estimate the strength of multiple competing ecological processes along with reliable confidence intervals, the current best approach is to use full (maximal) statistical models (possibly with Bayesian priors) after making principled, a priori decisions about model complexity.},
DOI = {10.3390/e26060506}
}

@article{zuur2010,
author = {Zuur, Alain F. and Ieno, Elena N. and Elphick, Chris S.},
title = {A protocol for data exploration to avoid common statistical problems},
journal = {Methods in Ecology and Evolution},
volume = {1},
number = {1},
pages = {3-14},
keywords = {collinearity, data exploration, independence, transformations, type I and II errors, zero inflation},
doi = {https://doi.org/10.1111/j.2041-210X.2009.00001.x},
url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/j.2041-210X.2009.00001.x},
eprint = {https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2041-210X.2009.00001.x},
abstract = {Summary 1. While teaching statistics to ecologists, the lead authors of this paper have noticed common statistical problems. If a random sample of their work (including scientific papers) produced before doing these courses were selected, half would probably contain violations of the underlying assumptions of the statistical techniques employed. 2. Some violations have little impact on the results or ecological conclusions; yet others increase type I or type II errors, potentially resulting in wrong ecological conclusions. Most of these violations can be avoided by applying better data exploration. These problems are especially troublesome in applied ecology, where management and policy decisions are often at stake. 3. Here, we provide a protocol for data exploration; discuss current tools to detect outliers, heterogeneity of variance, collinearity, dependence of observations, problems with interactions, double zeros in multivariate analysis, zero inflation in generalized linear modelling, and the correct type of relationships between dependent and independent variables; and provide advice on how to address these problems when they arise. We also address misconceptions about normality, and provide advice on data transformations. 4. Data exploration avoids type I and type II errors, among other problems, thereby reducing the chance of making wrong ecological conclusions and poor recommendations. It is therefore essential for good quality management and policy based on statistical analyses.},
year = {2010}
}

@book{mollick2025,
  title     = "Co-Intelligence: Living and Working with AI",
  author    = "Mollick, Ethan",
  year      = 2025,
  publisher = "Penguin Random House",
  address   = "London"
}

@article{zhu2024large,
  title={Are Large Language Models Good Statisticians?},
  author={Zhu, Yizhang and Du, Shiyin and Li, Boyan and Luo, Yuyu and Tang, Nan},
  journal={arXiv preprint arXiv:2406.07815},
  year={2024}
}



@article{gougherty2024,
    author = {Gougherty, Andrew V. and Clipp, Hannah L.},
    title = {Testing the reliability of an AI-based large language model to extract ecological information from the scientific literature},
    journal = {npj Biodiversity},
    volume = {3},
    number = {1},
    pages = {13},
    year = {2024},
    month = {5},
    doi = {10.1038/s44185-024-00043-9},
    url = {https://doi.org/10.1038/s44185-024-00043-9},
    abstract = {Artificial intelligence-based large language models (LLMs) have the potential to substantially improve the efficiency and scale of ecological research, but their propensity for delivering incorrect information raises significant concern about their usefulness in their current state. Here, we formally test how quickly and accurately an LLM performs in comparison to a human reviewer when tasked with extracting various types of ecological data from the scientific literature. We found the LLM was able to extract relevant data over 50 times faster than the reviewer and had very high accuracy (>90%) in extracting discrete and categorical data, but it performed poorly when extracting certain quantitative data. Our case study shows that LLMs offer great potential for generating large ecological databases at unprecedented speed and scale, but additional quality assurance steps are required to ensure data integrity.},
    issn = {2731-4243}
}

@article{spillias2024,
    title = {Human-AI collaboration to identify literature for evidence synthesis},
    author = {Spillias, Scott and Tuohy, Paris and Andreotta, Matthew and Annand-Jones, Ruby and Boschetti, Fabio and Cvitanovic, Christopher and Duggan, Joseph and Fulton, Elisabeth A. and Karcher, Denis B. and Paris, Cécile and Shellock, Rebecca and Trebilco, Rowan},
    journal = {Cell Reports Sustainability},
    volume = {1},
    number = {7},
    year = {2024},
    month = {7},
    publisher = {Elsevier},
    doi = {10.1016/j.crsus.2024.100132},
    issn = {2949-7906},
    url = {https://doi.org/10.1016/j.crsus.2024.100132}
}









@article{shoemaker2025,
author = {Kevin T. Shoemaker  and Kevin J. Loope },
title = {We need better ways to re-evaluate conservation policies when they’re founded on flawed research},
journal = {Proceedings of the National Academy of Sciences},
volume = {122},
number = {19},
pages = {e2426166122},
year = {2025},
doi = {10.1073/pnas.2426166122},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.2426166122},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2426166122}}

@inproceedings{onan2024assessing,
  title={Assessing the Impact of Prompt Strategies on Text Summarization with Large Language Models},
  author={Onan, Aytuǧ and Alhumyani, Hesham},
  booktitle={International Conference on Computer Applications in Industry and Engineering},
  pages={41--55},
  year={2024},
  organization={Springer}
}

@article{mondal2024evaluating,
  title={Evaluating large language models for selection of statistical test for research: A pilot study},
  author={Mondal, Himel and Mondal, Shaikat and Mittal, Prabhat},
  journal={Perspectives in Clinical Research},
  volume={15},
  number={4},
  pages={178--182},
  year={2024},
  publisher={Medknow}
}

@article{sivarajkumar2024empirical,
  title={An empirical evaluation of prompting strategies for large language models in zero-shot clinical natural language processing: algorithm development and validation study},
  author={Sivarajkumar, Sonish and Kelley, Mark and Samolyk-Mazzanti, Alyssa and Visweswaran, Shyam and Wang, Yanshan},
  journal={JMIR Medical Informatics},
  volume={12},
  pages={e55318},
  year={2024},
  publisher={JMIR Publications Toronto, Canada}
}

@article{ellis2023new,
  title={A new era of learning: Considerations for ChatGPT as a tool to enhance statistics and data science education},
  author={Ellis, Amanda R and Slade, Emily},
  journal={Journal of Statistics and Data Science Education},
  volume={31},
  number={2},
  pages={128--133},
  year={2023},
  publisher={Taylor \& Francis}
}

@article{jansen2025leveraging,
  title={Leveraging large language models for data analysis automation},
  author={Jansen, Jacqueline A and Manukyan, Art{\"u}r and Al Khoury, Nour and Akalin, Altuna},
  journal={PloS one},
  volume={20},
  number={2},
  pages={e0317084},
  year={2025},
  publisher={Public Library of Science San Francisco, CA USA}
}

@article{lin2024write,
  title={How to write effective prompts for large language models},
  author={Lin, Zhicheng},
  journal={Nature Human Behaviour},
  volume={8},
  number={4},
  pages={611--615},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{brown2018,
  title={Estimating the footprint of pollution on coral reefs with models of species turnover},
  author={Brown, Christopher and Hamilton, Richard},
  journal={Conservation Biology},
  volume={32},
  number={XX},
  pages={949--958},
  year={2018},
  publisher={Wiley}
}


@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{liao2024llms,
  title={LLMs as Research Tools: A Large Scale Survey of Researchers' Usage and Perceptions},
  author={Liao, Zhehui and Antoniak, Maria and Cheong, Inyoung and Cheng, Evie Yu-Yen and Lee, Ai-Heng and Lo, Kyle and Chang, Joseph Chee and Zhang, Amy X},
  journal={arXiv preprint arXiv:2411.05025},
  year={2024}
}

@article{Gilbert2024,
author = {Gilbert, Neil A. and Amaral, Bruna R. and Smith, Olivia M. and Williams, Peter J. and Ceyzyk, Sydney and Ayebare, Samuel and Davis, Kayla L. and Leuenberger, Wendy and Doser, Jeffrey W. and Zipkin, Elise F.},
title = {A century of statistical Ecology},
journal = {Ecology},
volume = {105},
number = {6},
pages = {e4283},
keywords = {data, history of science, model selection, quantitative ecology, statistical ecology, uncertainty},
doi = {https://doi.org/10.1002/ecy.4283},
url = {https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1002/ecy.4283},
eprint = {https://esajournals.onlinelibrary.wiley.com/doi/pdf/10.1002/ecy.4283},
abstract = {Abstract As data and computing power have surged in recent decades, statistical modeling has become an important tool for understanding ecological patterns and processes. Statistical modeling in ecology faces two major challenges. First, ecological data may not conform to traditional methods, and second, professional ecologists often do not receive extensive statistical training. In response to these challenges, the journal Ecology has published many innovative statistical ecology papers that introduced novel modeling methods and provided accessible guides to statistical best practices. In this paper, we reflect on Ecology's history and its role in the emergence of the subdiscipline of statistical ecology, which we define as the study of ecological systems using mathematical equations, probability, and empirical data. We showcase 36 influential statistical ecology papers that have been published in Ecology over the last century and, in so doing, comment on the evolution of the field. As data and computing power continue to increase, we anticipate continued growth in statistical ecology to tackle complex analyses and an expanding role for Ecology to publish innovative and influential papers, advancing the discipline and guiding practicing ecologists.},
year = {2024}
}

@article{kendall2019persistent,
  title={Persistent problems in the construction of matrix population models},
  author={Kendall, Bruce E and Fujiwara, Masami and Diaz-Lopez, Jasmin and Schneider, Sandra and Voigt, Jakob and Wiesner, S{\"o}ren},
  journal={Ecological modelling},
  volume={406},
  pages={33--43},
  year={2019},
  publisher={Elsevier}
}

@article{GIBERT20184,
title = {Environmental Data Science},
journal = {Environmental Modelling & Software},
volume = {106},
pages = {4-12},
year = {2018},
note = {Special Issue on Environmental Data Science. Applications to Air quality and Water cycle},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2018.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S1364815218301269},
author = {Karina Gibert and Jeffery S. Horsburgh and Ioannis N. Athanasiadis and Geoff Holmes},
keywords = {Data Science, Environmental science, Data driven modelling},
abstract = {Environmental data are growing in complexity, size, and resolution. Addressing the types of large, multidisciplinary problems faced by today's environmental scientists requires the ability to leverage available data and information to inform decision making. Successfully synthesizing heterogeneous data from multiple sources to support holistic analyses and extraction of new knowledge requires application of Data Science. In this paper, we present the origins and a brief history of Data Science. We revisit prior efforts to define Data Science and provide a more modern, working definition. We describe the new professional profile of a data scientist and new and emerging applications of Data Science within Environmental Sciences. We conclude with a discussion of current challenges for Environmental Data Science and suggest a path forward.}
}

@article{Culina2020,
    doi = {10.1371/journal.pbio.3000763},
    author = {Culina, Antica AND van den Berg, Ilona AND Evans, Simon AND Sánchez-Tójar, Alfredo},
    journal = {PLOS Biology},
    publisher = {Public Library of Science},
    title = {Low availability of code in ecology: A call for urgent action},
    year = {2020},
    month = {07},
    volume = {18},
    url = {https://doi.org/10.1371/journal.pbio.3000763},
    pages = {1-9},
    abstract = {Access to analytical code is essential for transparent and reproducible research. We review the state of code availability in ecology using a random sample of 346 nonmolecular articles published between 2015 and 2019 under mandatory or encouraged code-sharing policies. Our results call for urgent action to increase code availability: only 27% of eligible articles were accompanied by code. In contrast, data were available for 79% of eligible articles, highlighting that code availability is an important limiting factor for computational reproducibility in ecology. Although the percentage of ecological journals with mandatory or encouraged code-sharing policies has increased considerably, from 15% in 2015 to 75% in 2020, our results show that code-sharing policies are not adhered to by most authors. We hope these results will encourage journals, institutions, funding agencies, and researchers to address this alarming situation.},
    number = {7},

}

@article{popovic2024four,
author = {Popovic, Gordana and Mason, Tanya Jane and Drobniak, Szymon Marian and Marques, Tiago André and Potts, Joanne and Joo, Rocío and Altwegg, Res and Burns, Carolyn Claire Isabelle and McCarthy, Michael Andrew and Johnston, Alison and Nakagawa, Shinichi and McMillan, Louise and Devarajan, Kadambari and Taggart, Patrick Leo and Wunderlich, Alison and Mair, Magdalena M. and Martínez-Lanfranco, Juan Andrés and Lagisz, Malgorzata and Pottier, Patrice},
title = {Four principles for improved statistical ecology},
journal = {Methods in Ecology and Evolution},
volume = {15},
number = {2},
pages = {266-281},
keywords = {HARKing, model assumptions, p-hacking, pre-registration, p-values, questionable research practices, reproducibility crisis, research waste},
doi = {https://doi.org/10.1111/2041-210X.14270},
url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.14270},
eprint = {https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.14270},
abstract = {Abstract Increasing attention has been drawn to the misuse of statistical methods over recent years, with particular concern about the prevalence of practices such as poor experimental design, cherry picking and inadequate reporting. These failures are largely unintentional and no more common in ecology than in other scientific disciplines, with many of them easily remedied given the right guidance. Originating from a discussion at the 2020 International Statistical Ecology Conference, we show how ecologists can build their research following four guiding principles for impactful statistical research practices: (1) define a focussed research question, then plan sampling and analysis to answer it; (2) develop a model that accounts for the distribution and dependence of your data; (3) emphasise effect sizes to replace statistical significance with ecological relevance; and (4) report your methods and findings in sufficient detail so that your research is valid and reproducible. These principles provide a framework for experimental design and reporting that guards against unsound practices. Starting with a well-defined research question allows researchers to create an efficient study to answer it, and guards against poor research practices that lead to poor estimation of the direction, magnitude, and uncertainty of ecological relationships, and to poor replicability. Correct and appropriate statistical models give sound conclusions. Good reporting practices and a focus on ecological relevance make results impactful and replicable. Illustrated with two examples—an experiment to study the impact of disturbance on upland wetlands, and an observational study on blue tit colouring—this paper explains the rationale for the selection and use of effective statistical practices and provides practical guidance for ecologists seeking to improve their use of statistical methods.},
year = {2024}
}

@article{jenkins2023reproducibility,
  title={Reproducibility in ecology and evolution: Minimum standards for data and code},
  author={Jenkins, Gareth B and Beckerman, Andrew P and Bellard, C{\'e}line and Ben{\'\i}tez-L{\'o}pez, Ana and Ellison, Aaron M and Foote, Christopher G and Hufton, Andrew L and Lashley, Marcus A and Lortie, Christopher J and Ma, Zhaoxue and others},
  journal={Ecology and Evolution},
  volume={13},
  number={5},
  pages={e9961},
  year={2023},
  publisher={Wiley Online Library}
}


@article{messeri2024artificial,
  title={Artificial intelligence and illusions of understanding in scientific research},
  author={Messeri, Lisa and Crockett, MJ},
  journal={Nature},
  volume={627},
  number={8002},
  pages={49--58},
  year={2024},
  publisher={Nature Publishing Group UK London}
}


@misc{boonstra2024prompt,
  title={Prompt engineering},
  author={Boonstra, Lee},
  year={2024},
  url={https://www.kaggle.com/whitepaper-prompt-engineering},
  publisher={Google}
}

@article{gallo2024,
    author = {Gallo, Robert J and Baiocchi, Michael and Savage, Thomas R and Chen, Jonathan H},
    title = {Establishing best practices in large language model research: an application to repeat prompting},
    journal = {Journal of the American Medical Informatics Association},
    volume = {32},
    number = {2},
    pages = {386-390},
    year = {2024},
    month = {12},
    abstract = {We aimed to demonstrate the importance of establishing best practices in large language model research, using repeat prompting as an illustrative example.Using data from a prior study investigating potential model bias in peer review of medical abstracts, we compared methods that ignore correlation in model outputs from repeated prompting with a random effects method that accounts for this correlation.High correlation within groups was found when repeatedly prompting the model, with intraclass correlation coefficient of 0.69. Ignoring the inherent correlation in the data led to over 100-fold inflation of effective sample size. After appropriately accounting for this issue, the authors’ results reverse from a small but highly significant finding to no evidence of model bias.The establishment of best practices for LLM research is urgently needed, as demonstrated in this case where accounting for repeat prompting in analyses was critical for accurate study conclusions.},
    issn = {1527-974X},
    doi = {10.1093/jamia/ocae294},
    url = {https://doi.org/10.1093/jamia/ocae294},
    eprint = {https://academic.oup.com/jamia/article-pdf/32/2/386/60951519/ocae294.pdf},
}

@article{ji2025overview,
  title={An overview of large language models for statisticians},
  author={Ji, Wenlong and Yuan, Weizhe and Getzen, Emily and Cho, Kyunghyun and Jordan, Michael I and Mei, Song and Weston, Jason E and Su, Weijie J and Xu, Jing and Zhang, Linjun},
  journal={arXiv preprint arXiv:2502.17814},
  year={2025}
}

@article{zhou2024larger,
  title={Larger and more instructable language models become less reliable},
  author={Zhou, Lexin and Schellaert, Wout and Mart{\'\i}nez-Plumed, Fernando and Moros-Daval, Yael and Ferri, C{\`e}sar and Hern{\'a}ndez-Orallo, Jos{\'e}},
  journal={Nature},
  volume={634},
  number={8032},
  pages={61--68},
  year={2024},
  publisher={Nature Publishing Group UK London}
}


@article{king2022interactions,
  title={Interactions among multiple stressors vary with exposure duration and biological response},
  author={King, Olivia C and van de Merwe, Jason P and Campbell, Max D and Smith, Rachael A and Warne, Michael St J and Brown, Christopher J},
  journal={Proceedings of the Royal Society B},
  volume={289},
  number={1974},
  pages={20220348},
  year={2022},
  publisher={The Royal Society}
}

@article{fordham2018complex,
  title={How complex should models be? Comparing correlative and mechanistic range dynamics models},
  author={Fordham, Damien A and Bertelsmeier, Cleo and Brook, Barry W and Early, Regan and Neto, Dora and Brown, Stuart C and Ollier, S{\'e}bastien and Ara{\'u}jo, Miguel B},
  journal={Global Change Biology},
  volume={24},
  number={3},
  pages={1357--1370},
  year={2018},
  publisher={Wiley Online Library}
}

@article{touchon2016mismatch,
  title={The mismatch between current statistical practice and doctoral training in ecology},
  author={Touchon, Justin C and McCoy, Michael W},
  journal={Ecosphere},
  volume={7},
  number={8},
  pages={e01394},
  year={2016},
  publisher={Wiley Online Library}
}
