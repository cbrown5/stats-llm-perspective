--- 
title: "Perspective: Quality ecological statistics with large language models"
author: "CJ Brown (c.j.brown@utas.edu.au)"
date: "`r Sys.Date()`"
description: |
  Perspective article on prompting
github-repo: cbrown5/R-llm-workshop
bibliography: references.bib  
---

CJ Brown - c.j.brown@utas.edu.au
University of Tasmania

Scott Spillias
CSIRO Environment 

Perspective article

**Citation: Brown, Spillias (2025). Quality statistics with large language models. Unpublished manuscript**

Pre-print forthcoming soon... 

Aim for 2000-3000 words


<!--- Check journal word length, reduce by 500 words --->

<!--- Add idea for another display item --->

## Abstract

Large language models (LLMs) are rapidly transforming scientific workflows, including statistical analyses in ecological sciences. While these AI tools offer impressive capabilities for code generation and analytical guidance, evaluations reveal significant limitations in their reasoning for standard statistical tests. Ecological statistics typically require special consideration due to spatial and temporal structuring, so LLM performance on these tasks is likely to be worse than for other disciplines, such as clinical research. This perspective addresses the need for effective prompting guidelines to ensure quality statistical analyses when using LLMs. Drawing on empirical evaluations and practical experience, we provide a framework for ecological scientists to leverage these powerful tools while maintaining statistical rigor. Key recommendations include: separating workflows into components that align with LLM strengths and limitations; providing rich context through domain knowledge, data summaries, and research questions; combining rich context with structured prompting techniques like Chain of Thought reasoning; and maintaining human oversight of statistical decisions. By understanding LLM capabilities and employing these prompting strategies, researchers can harness these technologies to improve rather than compromise statistical quality in ecological research. Future research should focus on evaluations of LLMs for ecological statistics, development of specialized prompting strategies, and integration of LLMs with traditional statistical approaches.

## Introduction

Large language models (LLMs) are rapidly transforming scientific workflows, with profound implications for statistical analysis in environmental sciences. Recent surveys indicate that over XXX% of researchers now incorporate LLMs into their workflows, with many specifically using them for statistical advice and code generation. The appeal is clear: LLMs almost instantly generate statistical code and analyses that would traditionally require extensive training and time to develop. For example, researchers can now produce a complete bioinformatics analysis analysis—including code and visualizations [@jansen2025leveraging] in under 15 minutes. LLMs can also interpret statistics and figures to produce written results.

The efficiency of relying on LLMs for statistics comes with significant risks. Recent evaluations reveal concerning limitations in LLMs' statistical reasoning abilities. One study found that accuracy at suggesting appropriate statistical tests was typically below 40% for anything beyond basic descriptive statistics [@zhu2024large]. Crucially, the quality of statistical advice from LLMs depends heavily on how questions are framed [@onan2024assessing; @zhu2024large; @jansen2025leveraging] — effective prompting strategies can almost double the accuracy of recommendations. Present evaluations of LLMs are focused on clinical research, generally where samples are independent. It is likely that their performance is significantly poorer for the complex depenence structures and observation patterns that are common in ecology. 

This perspective article addresses the urgent need for guidelines on using LLMs for statistical analysis in ecological research. As LLM adoption outpaces formal evaluation, we cannot wait for comprehensive peer-reviewed assessments before establishing best practices. Drawing on empirical evaluations, practical experience, and broader AI literature, we provide a framework for leveraging these powerful tools while maintaining statistical rigor. By understanding LLM capabilities and limitations and employing structured prompting strategies, researchers can harness these technologies to enhance rather than compromise statistical quality in environmental research. The guidelines presented here aim to help environmental scientists navigate this rapidly evolving landscape responsibly and effectively.

## Challenges for statistical analysis quality in environmental sciences

Statistical analysis in ecological sciences faces numerous challenges that predate the emergence of LLMs but may be exacerbated by their use. Modern data analysis requires two interrelated skills: computer programming and statistical reasoning. There exists a substantial gap between specialists at the forefront of statistical computing and experts in specific ecological disciplines who use statistics irregularly. Environmental data often violate standard statistical assumptions, requiring specialized analytical approaches that may not be well-represented in the text that LLMs are trained on. Ecological analyses may also require advanced computer programming skills where it is easy to make mistakes. 

Lack of statistical training among environmental scientists has long undermined research quality and application. Reproducibility is a wide-spread issue and ecology is no exception. P-hacking and other forms of bias caused by manipulating analyses after viewing results are already prevalent in ecology and evolution, often justified by researchers as necessary for career survival [@fraser2018; @forstmeier2017]. 

Accidental statistical mistakes caused by inappropraite training or misguided conventions are also an issue. Common misapplications include inappropriate transformations of response variables [@ohara2010], applying methods that assume independent samples to time-series analysis [@brown2011], using linear regression for zero-inflated data [@warton2016], conflating prediction with causality [@arif2022], and inappropriate use of multi-model averages [@bolker2024]. 

Problems can also arise from flawed implementation of ecological analysis. 

<!--- Add citation to Bruce Kendall study, cited in shoemaker B. E. Kendall et al., Persistent problems in the construction of matrix population models. Ecol. Model. 406, 33–43 (2019). --->
These errors are not merely academic concerns—they can lead to misinformed policy actions with real consequences for conservation outcomes [@shoemaker2025].

## Applications and Risks of LLMs in Environmental Statistics

Large language models present both significant opportunities and challenges for statistical practice in environmental sciences. When used with appropriate guidance and oversight, these AI tools can enhance research workflows, but they also introduce risks that require careful consideration.

### Opportunities for Enhanced Statistical Practice

LLMs may help bridge the gap between statistical experts and those applying statistics to their field of ecology by simultaneously democratizing access to statistical expertise and computer programming. LLMs can democratize access to statistical expertise, providing researchers who lack ready access to statistical collaborators with guidance on appropriate methods and implementation strategies. This democratization is particularly valuable in resource-constrained settings or for early-career researchers still developing their statistical abilities. For example, researchers from institutions without dedicated statistical support can leverage LLMs to explore analytical options that might otherwise be inaccessible.

<!--- ADD REFERENCE HERE ON DEMOCRACY OF ANALYSIS??? is this a thing?? --->

Documentation and transparency are an ongoing challenge for statistical ecology. 
When used correctly, LLMs can produce well-documented code with comprehensive comments, logical structure, and adherence to style guides—potentially improving computational reproducibility across environmental sciences. This advantage addresses a persistent challenge in the field, where poorly documented or inconsistently structured code has hampered reproducibility efforts.

<!--- ADD REFERENCE HERE ON REPRODUCIBILITY IN ECOLOGY AND CODING --->

LLMs also facilitate rapid exploration of alternative analytical approaches, enabling researchers to quickly generate and compare multiple statistical strategies. This capability could support more robust sensitivity analyses, as researchers can efficiently implement various models to assess how analytical choices influence results. For instance, an ecologist studying species distributions could use an LLM to implement both frequentist and machine learning approaches to the same question, comparing outcomes without investing extensive time in coding each approach from scratch.

<!--- EDD REFERENCE HERE ON LACK OF SENSITIVITY ANALYSIS IN ECOLOGY AND CODING --->

Beyond implementation benefits, LLMs offer valuable opportunities for statistical learning and skill development. When used as interactive tutors rather than black-box solution providers, these models can enhance researchers' statistical understanding by explaining concepts, suggesting relevant literature, and demonstrating proper implementation techniques [@ellis2023new].

### Risks and Limitations

Despite these benefits, LLMs present several specific risks for statistical practice that require careful mitigation strategies.

First, LLMs may amplify existing problems with statistical quality. By dramatically accelerating the ability to try multiple analytical approaches, LLMs could enable unprecedented levels of p-hacking and selective reporting. Researchers can now explore tens or hundreds of alternatives for solving a statistical issue in minutes, creating far more opportunities to cherry-pick favorable results. Strong research reporting standards and ethics are ultimately needed to combat this issue.

Second, LLMs exhibit overconfidence in their statistical recommendations. For example, they perpetuate common misunderstandings of confidence intervals and p-values [@ellis2023new] (as of Claude 4.0 this was still true). They almost always provide an answer, typically with high apparent certainty, even when their suggestions are inappropriate or incorrect. This characteristic is particularly problematic in environmental sciences, where data often have complex structures requiring specialized approaches. Current LLMs may not adequately recognize or account for these nuances of environmental data.

Third, LLMs lack true statistical understanding. Unlike traditional statistical software that implements specific algorithms, LLMs generate responses based on patterns learned from training data, they do not "understand" statistics and cannot reason in the way human experts do. They work by predicting statistically likely responses to text - therefore the challenge is to use prompts that shift the distribution of likely response to better overlap with accurate responses. This fundamental limitation means they may confidently suggest inappropriate methods, apply internally consistent logic to the wrong question, fail to recognize violations of statistical assumptions, or generate plausible-sounding but incorrect interpretations.

Fourth, inexperienced users may be particularly vulnerable to these risks [@ellis2023new]. Without sufficient statistical background to critically evaluate LLM suggestions, researchers might implement inappropriate analyses or misinterpret results. The apparent authority and confidence of LLM responses can create a false sense of security, potentially leading to erroneous conclusions that influence scientific understanding and policy decisions.

Finally, there is a risk of statistical deskilling in the research community. If researchers increasingly rely on LLMs for statistical decisions without developing their own understanding, the collective statistical literacy of the field could decline over time. This would create a dangerous dependency on tools that lack true statistical reasoning capabilities.

### Toward Effective Human-AI Statistical Partnerships

The challenge is to develop workflows that maximize LLMs' strengths while compensating for their weaknesses. This requires providing sufficient context about research questions, data characteristics, and analytical constraints to guide the model toward appropriate statistical recommendations. It also involves maintaining oversight of model outputs, particularly for decisions requiring deeper statistical understanding such as model formulation, assumption checking, and result interpretation.

The opportunity lies in developing a statistical workflow that combines human expertise with LLM capabilities. In this workflow, researchers maintain responsibility for statistical decisions while using LLMs to implement analyses efficiently, explore options, and enhance documentation. This human-AI partnership represents a middle path between complete automation and traditional manual implementation—leveraging the efficiency and consistency of LLMs while preserving the critical judgment and domain expertise of human researchers. The key to this partnership is effective prompting—providing LLMs with the context, constraints, and guidance needed to generate high-quality statistical advice and code that advances rather than compromises statistical rigor in environmental research.

## LLM Overview

To develop effective prompting strategies, it's essential to understand how LLMs function. At their core, LLMs are prediction engines that generate text one token at a time based on patterns learned during training. A token is roughly equivalent to a word part, a word, or a common phrase.

Several key parameters influence LLM behavior:

1. **Temperature**: Controls randomness in token prediction. Lower temperatures (closer to 0) make responses more deterministic and conservative, while higher temperatures (greater than 1.0) increase creativity but potentially reduce reliability. For statistical applications, lower temperatures typically produce more consistent and conventional recommendations.

2. **Context window**: The amount of text an LLM can consider when generating a response. Current LLMs have context windows typically in the range from 100,000 to 2,000,000 tokens. Larger context windows allow for including more detailed information about data, research questions, and statistical requirements.

3. **Model complexity**: Different models have varying capabilities based on their size, training data, and architecture. More complex models (e.g., Claude-4.0-Opus vs. Claude-4.0-Sonnet) generally provide more nuanced statistical guidance but at higher computational and financial cost. 

4. **System prompt**: Sets the overall context and constraints for the LLM. This "behind-the-scenes" instruction shapes how the model responds to user queries and can significantly impact statistical advice quality.

5. **AI assistant, AI programmer pair**: Software that assists a user to interact with an LLM. Examples include Github Copilot and Claude Code. This software manages user interactions, including setting the system message (which may be proprietary information) and managing the context window. 

6. **Tools and MCP**: Tools allow LLMs to perform tasks. Examples include running R scripts, searching the internet and downloading online data. A common standard for tool definition is the Model Context Protocol (MCP). 

7. **Agents**: Agents are software systems that allow LLMs to iteratively develop their own task, with or without human supervision. For instance, an agent can have a tool allowing it to run and read terminal commands. This lets the agent write R scripts, run them, check for errors, and correct iteratively. Agents are most commonly used within AI assistant software like Github Copilot, thought there is some development of agents for bespoke statistical problems [@jansen2025leveraging]. 

## Prompting Guidelines Best Practices

There are now many formal evaluations of LLMs for statistical advice. However, many of these studies are not replicable and do not follow statistical best practice. For instance, they do not provide the prompts they used, do not replicate prompts (LLM responses differ ever time) or use statistics that inflate estimates of effect size (ref - mixed effects model reference). Here we summarize the handful of evaluations that provide sufficient information to assess the scientific credibility of their claims. 

The key findings of these studies are that more accurate responses are obtained when: 

- Role prompting is used, e.g. `You are an expert in the statistical analysis of ecological data`  [@jansen2025leveraging]
- Examples and reference material are included (also called one-shot or few-shot prompting) [@zhu2024large] 
- Context about the data collection process is included [@zhu2024large]
- The data are attached as part of the prompt [@jansen2025leveraging; @zhu2024large]

Providing examples that pair types of statistical questions with appropriate solutions one of the most effective approaches to improve the precision of responses [@sivarajkumar2024empirical; @zhu2024large]. This approach should be used whereever possible, however, accurate examples may not be readily available to the novice statistician. Reference material can also be provided in place of examples. For example, a user could attach a blog or package vignette that illustrates the application of an analysis to answering a research question. 

A further tactic, 'chain of thought' reasoning, has mixed success. Chain of thought reasoning encourages the model to structure its prompt as in a step-by-step way and tends to improve the quality of reasoning (ref??). It can be as sample as adding to a prompt `Use chain of thought reasoning`. Its utility has mixed performance for statistical analyses [@jansen2025leveraging; @zhu2024large]. Chain of thought prompting is best combined with prompts that include the data and measurement context. 

Prompts that say what to do, rather than what not to do, are generally also considered be more effective. 

### Recognize different steps in workflows

It is helpful to separate statistical workflows into distinct components that align with LLM strengths and limitations:

1. **Statistical approach selection**: Determining appropriate statistical methods for research questions
2. **Implementation planning**: Designing the analytical workflow and code structure
3. **Code generation**: Writing the actual code to implement analyses
4. **Interpretation guidance**: Understanding and reporting results

We deal with steps 1-3 here. The credibility of statistical interpretation urgently needs further empirical evaluation, so we leave that for future studies.  

LLMs perform differently across these components. They excel at code generation and implementation planning but are less reliable for selecting appropriate statistical approaches or interpreting complex results. 

LLMs can be used across all of these steps, but we recommend that each step is treated separately. This encourages informed decision making and avoids making decisions on the fly. For instance, it is better to design the statistical analysis prior to setting an agent up to automate the implementation of that analysis. 

The separation of workflow steps also helps prevent overreliance on LLMs for statistical decisions. 

An example would be to first prompt the LLM to ask for statistical analyses that can address a particular issue. Once the analysis is resolved, then the user can prompt for R packages to perform the analysis, then prompt to generate the code. 

### Be clear about your hypotheses and provide domain knowledge

This example is written from the point of view of a researcher who knows their hypothesis and data, but is unclear about what methods they should use. 

```
How can I statistically test the dependence of fish abundance on coral cover? 
Abundances are count data representing, and the cover variable is proportional data. Abundances were measured at 50 different locations. Search the web to find robust recommendations for ecologists to analyze count data before proceeding with your recommendations.
```

In this prompt we made use of several strategies. First we were clear about the hypothesis. We gave context about the data and sampling methodology, by telling the LLM we had count data measured across different locations. Doing so ensures that the response is more likely to include count appropriate methods (e.g. Poisson GLM). Finally, we asked the LLM to use a web search to find domain knowledge in our field (this assumes you are using an AI assitant that has a web search tool available). 

### Provide context on experimental/observational design 

Extending the previous example we could also provide context about survey design: 

```
I need to analyze the dependence of fish abundance (integer counts, 
zero-inflated) on coral cover (continuous percentage). Sites are spatially 
clustered within regions. What statistical approaches would be appropriate?
```

In this prompt by including the context that sites are spatial replicates the LLM is more likely to recommend methods that account for spatial structure in the data. 

### Attach data

A consistent finding of formal evaluations of LLMs for statistical advice is to attach the data 

```
How can I statistically test the depdence of `pres.topa` on `CB_cover`? 
Here are the first 6 rows of data:
[data table]
```

Here we have specifically named the variables as they appear in the dataset (e.g. `pres.topa`). This ensures precision reference to the target data. We have also dragged and dropped the first 6 rows of the data into the prompt (this is possible with many assistants, e.g. github copilot). Often (but not always) the LLM will appropriately recognize the response as a count variable if the data is attached. 

Similarly, data summaries can also be included in the prompt to further improve the context. 

### Use Chain of Thought reasoning with reputable source attached 

```
Using Chain of Thought reasoning, what statistical approach would be most 
appropriate for analyzing the relationship between fish abundance (count data) 
and coral cover (continuous)? We have attached a guideline for analysis of count data in ecology: [attach reference]
```

Chain of Thought reasoning is a common and easy prompting strategy that can dramatically improve responses in many domains, particularly reasoning. This works because many forms of LLMs 'think' by writing (a trait they share with good researchers). 

Chain of thought does not improve statistical advice on its own [@zhu2024large]. If used on its own, CoT will cause the LLM to elaborate on the same incorrect suggestions. In this prompt we dragged and dropped a reference (e.g. a blog from a reputable source, or a vignette from a popular ecological modelling R package). Therefore, the LLM could reason accurately with its improved domain knowledge. More recently, 'reasoning' models incorporate this approach natively and will respond to user queries with a stream of content that reflects the LLM's 'thoughts' before providing a response. 

For implementation of analysis, chain of thought can be effective, because it helps break up the workflow into a series of smaller steps. 

### Request self-evaluation and multiple options** 

```
Evaluate the robustness of each suggestion on a 1-10 scale and explain 
the strengths and limitations of each approach.
```

or 

```
What are three different statistical approaches I could use for this analysis? 
For each, explain the assumptions, advantages, and limitations.
```

These suggestions are related to CoT, and promote further thinking. Once again, best used in a chain of prompts when you have attached a reputable reference. Note that most LLM assistants have the imperitive to provide options baked in by the providers, so it is usually not neccessary to explictly ask for options. 

### Prompt bootstrapping 

The above prompts provide simple outlines. In practice, it is generally better to give as much context about the data and research question as possible. Users can bootstrap prompts. The user would start with a simpler prompts, such as those above, and then get the LLM to draft a more complete prompt, which the user can then edit. 

```
I'm trying to improve a prompt asking an assistant for statistical advice. Here is the prompt [prompt]. What other information can I provide to improve the accuracy of the asssitant's response? 
```

### Organized and modular project set-up

LLMs are most effectively if used via an AI assistant like github copilot. These assistants have access to context from your project files and can edit directly into documents and scripts. 

We recommend users carefully organize their their code and analysis. For example, a typical project directory may look like this: 

```
my-project/
├── README.md 
├── .gitignore
├── Scripts/ # R code
│   ├── 01_data-prep.R
│   ├── 02_data-analysis.R
│   └── 03_plots.R
├── Shared/       
│   ├── Outputs/
│   │   ├── Figures/
│   │   ├── data-prep/
│   │   └── model-objects/
│   ├── Data/
│   └── Manuscripts/   
```

Scripts should be kept short and modula. We find that ecologists typically write long scripts that span data wrangling to plotting and  may not even evaluate in a top to bottom order! 

Keeping your project modular organized allows for agents to easily navigate your project. It also lets you point precisely to files that may need attention. Time can be saved on the menial task of setting up and maintaining documentation by prompting an agent to do it for you.  

If using an agent to implement the analysis, be sure to include a request like: 

```
Create modular scripts for this analysis with separate files for data
preparation, model fitting, diagnostics, and visualization.
```

Modular code organization significantly enhances reproducibility and maintainability of statistical analyses, particularly for complex environmental data. 

For environmental analyses that often involve multiple stages—data cleaning, transformation, model fitting, diagnostics, and visualization—this modular structure is particularly valuable. LLMs excel at generating this type of organized code when explicitly prompted, but may default to monolithic scripts if not specifically directed. Request separate functions or files for each major analytical step, with clear documentation of inputs, outputs, and dependencies between components. This structure not only improves immediate code quality but also enhances long-term project sustainability and knowledge transfer.

### Provide implementation constraints

 Specify packages, coding style, and other requirements:

```
Implement this analysis using the tidyverse ecosystem and INLA for Bayesian 
mod
eling. Follow tidyverse style guidelines and prioritize code readability.
```
### Include statistical verification steps

 Request code that validates assumptions and checks model fit:

```
Include diagnostic checks for overdispersion, zero-inflation, and spatial 
autocorrelation in the model implementation.
```

### Iteratively refine your analysis and implementation 

Start with a basic implementation and progressively add complexity:

```
Let's start with a simple negative binomial GLM for fish abundance. Once 
that's working, we'll extend it to account for spatial clustering.
```

Agents can also self correct implementation errors. Allowing the agent to iterate through running and debugging a script >3 times can improve the accuracy of code syntax many times [@jansen2025leveraging]. 

### LLM reviewer 

Use an LLM to simulate peer-review of your statistical approach: 

```
Review my choice of analysis from the perspective of a peer-reviewer in an ecological journal
```

### Write longer prompts that are rich in context

All of the above advice suggests using longer and more detailed prompts than we typically see our colleagues using. LLM's have context windows that allow 100,000s of tokens equivalent to several theses worth of work. Our experience is that most researchers are under-using this potential for detailed prompts. 

One approach is to include all of the project information in a detailed project readme file (we prefer markdown format for this file, see supplemental material). The readme file can be developed iteratively with an LLM's help. In our readme file we include: 

- Research context
- Research aims
- Analysis methodology
- Tech context including R package preferences
- Analysis steps
- Directory structure
- Data file locations and meta-data 

The readme can be attached to every prompt to provide project context, as well as give the LLM a memory across different chat sessions. 

## Discussion and Conclusion

Large language models represent both opportunity and challenge for statistical practice in environmental sciences. When used thoughtfully with effective prompting strategies, they can enhance analytical workflows, improve code quality, and potentially address longstanding issues in statistical implementation. However, uncritical reliance on LLMs risks perpetuating or even amplifying existing problems in statistical practice.

The prompting guidelines presented in this perspective provide a framework for leveraging LLMs while maintaining statistical rigor. By separating workflows into components that align with LLM strengths and limitations, providing appropriate context and constraints, and maintaining human oversight of critical decisions, researchers can harness these powerful tools while mitigating their risks.

Several key principles emerge from this analysis:

1. **Maintain critical thinking**: LLMs should complement rather than replace statistical expertise. Researchers must critically evaluate LLM suggestions against domain knowledge and statistical principles.

2. **Provide rich context**: The quality of LLM statistical guidance improves dramatically when provided with detailed information about research questions, data characteristics, and analytical constraints.

3. **Document LLM use**: Transparency about LLM use in research workflows is essential for reproducibility and evaluation. Publications should clearly describe that LLMs were used.

4. **Develop LLM literacy**: As these tools become increasingly integrated into research workflows, developing "LLM literacy"—understanding how these models work, their limitations, and effective interaction strategies—becomes an essential skill for environmental scientists.

The rapid evolution of LLM capabilities suggests that their role in statistical workflows will only increase. Current models already show impressive performance in code generation and implementation planning, and future models may address some of the limitations identified in statistical reasoning. However, the fundamental nature of LLMs as prediction engines rather than reasoning systems means that human oversight will remain essential for ensuring statistical quality.

### Research Needs

Research is needed to inform the appropriate use of LLMs in ecological statistics. We identify several priorities. First, we need formal evaluations of LLM statistical performance for ecological datasets and problems. Ecological data presents unique problems for statistical analysis and it is not yet clear how reliable LLMs advice on these problems is. 

Second, we need to develop prompting templates that novice analysts and statistical coders can use to reliably develop their analyses. These could include the recommendations above, as well as reference material that is attuned to specific types of ecological data. 

Finally, LLMs have certain biases (refs), but it is not yet clear if there are important implications for ecological analyses.  Further research is needed to understand how LLM use may bias analyses in harmful ways. 

By addressing these research needs and adopting thoughtful prompting strategies, environmental scientists can harness the power of large language models to enhance rather than compromise statistical quality. The future of environmental statistics likely lies not in choosing between human expertise and artificial intelligence, but in developing effective partnerships that leverage the unique strengths of each.

## Acknowledgements

CJB was supported by a Future Fellowship (FT210100792) from the Australian Research Council. 

## Figures

<!-- UPTDATE FIGURE --> 
```{r workflow-diagram, fig.width=5, fig.height=3, dpi=300, echo = FALSE}
library(DiagrammeR)

DiagrammeR::grViz("
digraph workflow {
  # Graph settings
  graph [rankdir = TD, fontname = 'Arial', nodesep = 0.8, ranksep = 0.8]
  node [shape = rectangle, fontname = 'Arial', fontsize = 12, style = 'filled', 
        fillcolor = 'white', margin = 0.2]
  edge [fontname = 'Arial', fontsize = 10]

  # Main workflow steps
  step1 [label = 'Step 1: Statistical Approach Selection', fillcolor = '#e6f3ff', width = 3, height = 0.8]
  step2 [label = 'Step 2: Planning Implementation', fillcolor = '#e6f3ff', width = 3, height = 0.8]
  step3 [label = 'Step 3: Code Generation', fillcolor = '#e6f3ff', width = 3, height = 0.8]
  
  # Recommendations for each step
  rec1 [label = 'Recommendations:\\n• Explore multiple analytical approaches\\n• Provide domain knowledge and relevant literature\\n• Combine domain knowledge with step-by-step reasoning (Chain of Thought)\\n• Compare statistical approaches with alternatives\\n• Verify against statistical textbooks and guidelines', 
        shape = 'box', fillcolor = '#f0f8ff', width = 6]
        
  rec2 [label = 'Recommendations:\\n• Create a detailed README with analysis context\\n• Define clear research questions and hypotheses\\n• Create an organized and modular project directory\\n• Specify data characteristics and structure\\n• Outline analytical constraints and assumptions\\n• Plan workflow before implementation', 
        shape = 'box', fillcolor = '#f0f8ff', width = 6]
        
  rec3 [label = 'Recommendations:\\n• Use a two-step approach: plan then implement\\n• Provide implementation constraints (packages, style)\\n• Request modular, well-documented code\\n• Include verification and diagnostic steps\\n• Iteratively refine from simple to complex models\\n• Add comments explaining statistical and implementation \n• Define clear research questions and hypotheses', 
        shape = 'box', fillcolor = '#f0f8ff', width = 6]

  # Connections between steps
  step1 -> step2 -> step3 [weight = 5]
  
  # Connect steps to their recommendations
  step1 -> rec1 [dir = none, style = dashed]
  step2 -> rec2 [dir = none, style = dashed]  
  step3 -> rec3 [dir = none, style = dashed]
  
  # Human oversight element
  {rank = same; human [label = 'Human Oversight\\nand Evaluation', 
                      shape = 'oval', fillcolor = '#ffffcc', 
                      style = 'filled,dashed', width = 3]}
                      
  human -> step1 [dir = both, style = dashed, constraint = false]
  human -> step2 [dir = both, style = dashed, constraint = false]
  human -> step3 [dir = both, style = dashed, constraint = false]
}
")
```

**Figure 1**
Recommended workflow for using LLMs in statistical analysis, showing the four key steps alongside specific recommendations for effectively leveraging LLMs at each stage while maintaining scientific rigor.

## References
